{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c156efbe",
   "metadata": {},
   "source": [
    "![Portada](https://drive.google.com/uc?id=1-PwHqBvvDN777S8ua4KAv4iYf_u5N3Hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55c680",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "En este proyecto se realiza la implementación de un simulador de entorno para medir el rendimiento de un agente \"aspiradora\".El objetivo del simulador es medir el rendimiento del agente (aspiradora), dado diferentes configuraciones y parámetros. El simulador debe ser modular, lo que significa que debe estar diseñado de tal manera que diferentes componentes del sistema puedan ser fácilmente cambiados sin afectar el resto del código. Por ejemplo, los sensores, actuadores y características ambientales como el tamaño, la forma y la ubicación de la suciedad deben ser fácilmente configurables. En general, este ejercicio tiene como reto crear un entorno de simulación flexible y personalizable para probar y evaluar el rendimiento de un agente \"aspiradora\" en un mundo específico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50697e5b",
   "metadata": {},
   "source": [
    "## I. Introduccion\n",
    "  Un agente se puede definir como un sistema que percibe su entorno a través de sensores y actúa sobre él a través de actuadores. Esto puede ser ejemplificado por varios agentes, como un agente humano con ojos y oídos como sensores y manos y piernas como actuadores, un agente robótico con cámaras y telémetros infrarrojos como sensores y motores como actuadores, o un agente de software que recibe pulsaciones de teclas, contenidos de archivos y paquetes de red como entradas y visualizaciones en la pantalla, escribe archivos y envía paquetes de red como salidas. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Q_IF-fw-QV9gUXym8I2ZDIZSu5kv1nRf\" alt=\"agent\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "De acuerdo a la descripción del libro [1], se pueden dar las siguientes características de agente en la Figura 2.2.\n",
    "\n",
    "a) El **entorno** es una cuadrícula de dos dimensiones dividida en el cuadrado A y B, cada una de las cuales puede estar sucia o no.\n",
    "\n",
    "b) Las **percepciones** son consisten en si la ubicación actual está sucia o no.\n",
    "\n",
    "c) Posee un simple **sensor** el cual detecta si la ubicación actual está sucia o no.\n",
    "\n",
    "d) El **actuador** es la capacidad del agente para moverse por el entorno y limpiar un lugar si está sucio. \n",
    "\n",
    "\n",
    "Por lo tanto, en esta asignacion se pretende responder a la pregunta clave:\n",
    "\n",
    "**¿Cómo diseñar una implementación general que mida el desempeño de una aspiradora en un entorno bidimensional?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9307024",
   "metadata": {},
   "source": [
    "## II. Ejercicio 2.8 del libro de texto de Russell & Norvig's 3ª edición.\n",
    "\n",
    "\n",
    "En este proyecto se realiza la implementación de un simulador de entorno para medir el rendimiento de un agente \"aspiradora\" en un mundo específico descrito en la Figura 2.2 del libro de texto de Russell & Norvig 3ª edición [1].\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=14aUg5UHE_nt469znamvV8-rmyY9njXrt\" alt=\"Vacuum-world\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "El objetivo del simulador de entorno es medir el rendimiento del agente (aspiradora), dado diferentes configuraciones y parámetros.\n",
    "\n",
    "El simulador debe ser modular, lo que significa que debe estar diseñado de tal manera que diferentes componentes del sistema puedan ser fácilmente cambiados sin afectar el resto del código. Por ejemplo, los sensores, actuadores y características ambientales como el tamaño, la forma y la ubicación de la suciedad deben ser fácilmente configurables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa36c9",
   "metadata": {},
   "source": [
    "En general, este ejercicio tiene como reto crear un entorno de simulación flexible y personalizable para probar y evaluar el rendimiento de un agente \"aspiradora\" en un mundo específico. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f90c5",
   "metadata": {},
   "source": [
    "### A. Implementación\n",
    "Dado que para el lenguage Python existe una implementación ya realizada en el [repositorio de códigos de UC Berkeley](https://github.com/aimacode) [2], nos concentramos en entender el problema y el código implementado, ahora explicaremos como funciona o trabaja dicha implementación.\n",
    "\n",
    "\n",
    "El archivo **agent.py** implementa un módulo en Python, que contiene la implementación de varios tipos de agentes para diferentes entornos, correspondientes al problema de la Figura 2.2, como ser:\n",
    "\n",
    "a) Agentes reflejos simples (clase ReflexVacuumAgent)\n",
    "b) Agentes reflejos basados en modelos (clase ModelBasedVacuumAgent)\n",
    "c) Agentes basados en objetivos (clase GoalBasedAgent)\n",
    "d) Agentes basados en utilidades (clase UtilityBasedAgent)\n",
    "e) Agentes de aprendizaje (clase LearningAgent)\n",
    "\n",
    "Esta implementacion también proporciona otras clases y funciones para implementar y simular agentes en varios entornos. A continuación se detallará de manera breve estas clases y funciones.\n",
    "\n",
    "Clase **Agent**:  proporciona una clase base para todos los agentes, con métodos y propiedades básicos.\n",
    "\n",
    "Clase **VacuumEnvironment**: representa el entorno de la aspiradora, contiene métodos para agregar agentes, mover agentes y limpiar la suciedad.\n",
    "\n",
    "Clase **Environment**: es una clase base para todos los entornos y define los métodos y propiedades básicos que debe tener un entorno.\n",
    "\n",
    "Clase **GraphEnvironment**: representa un entorno basado en gráficos y proporciona métodos para agregar nodos y bordes al gráfico.\n",
    "\n",
    "Clase **TrivialVacuumEnvironment**: representa un entorno de aspiradora simple con una sola ubicación. Se puede utilizar para probar agentes reflejos simples como el TrivialVacuumAgent.\n",
    "\n",
    "Clase ***TrivialVacuumAgent**: representa un agente reflejo simple para TrivialVacuumEnvironment. Se puede utilizar para probar la funcionalidad básica de VacuumEnvironment.\n",
    "\n",
    "Clase **ReflexVacuumAgent**: representa un agente reflejo simple para el entorno de la aspiradora, con un proceso de toma de decisiones basado en reglas. Se puede utilizar para probar el rendimiento del agente en diferentes entornos.\n",
    "\n",
    "Clase **ModelBasedVacuumAgent**: representa un agente reflejo basado en modelos para el entorno de la aspiradora, con un proceso de toma de decisiones basado en el estado. Se puede utilizar para probar el rendimiento del agente en diferentes entornos.\n",
    "\n",
    "Clase **TableDrivenVacuumAgent**: representa un agente basado en tablas para el entorno de la aspiradora, con un proceso de toma de decisiones basado en una tabla de búsqueda. Se puede utilizar para probar el rendimiento del agente en diferentes entornos.\n",
    "\n",
    "Clase **RandomVacuumAgent**: representa un agente aleatorio para el entorno de la aspiradora, con un proceso de toma de decisiones basado en acciones aleatorias. Se puede utilizar para probar el rendimiento del agente en diferentes entornos.\n",
    "\n",
    "Clase **VacuumWorld**: representa un entorno de la aspiradora basado en cuadrículas, con métodos para agregar agentes, mover agentes y limpiar la suciedad.\n",
    "\n",
    "Clase **Direction**: define las direcciones posibles para el movimiento en un entorno basado en cuadrícula.\n",
    "\n",
    "Clase **Grid**: representa una cuadrícula en un entorno basado en cuadrícula, con métodos para agregar y eliminar objetos, obtener ubicaciones de objetos y verificar colisiones de objetos.\n",
    "\n",
    "clase **GridEnvironment**: representa un entorno basado en cuadrículas, con métodos para agregar agentes, mover agentes y limpiar la suciedad.\n",
    "\n",
    "Clase **XYEnvironment**: representa un sistema de coordenadas cartesianas bidimensional, con métodos para agregar y eliminar objetos, obtener ubicaciones de objetos y verificar colisiones de objetos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6879d",
   "metadata": {},
   "source": [
    "El ambiente para el agente de reflejo simple mostrado en la Figura 2.2 se implementó en la clase **TrivialVacuumEnvironment**, esta clase modela un entorno de dos estados con dos ubicaciones, cada una de las cuales pueden estar limpias o sucias. También define las acciones que puede realizar el agente, las cuales son: moverse hacia la izquierda, derecha limpiar y no operar ('Right', 'Left', 'Suck', 'NoOp'). En este entorno el agente no tiene un razonamiento, por lo que las acciones que toma las realiza de manera aleatoria. Las acciones moverse hacia la izquierda, derecha son penalizadas con un -1, mientras que la acción limpiar es premiada con un +10 siempre y cuando la ubicación se encuentre sucia. Por otro lado, si se realiza una limpieza en una posición limpia o si no se opera la penalización es de 0. Esto se puede observar con más detalle en el fragmento de programa de abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a440f27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2022 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2022 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #ffffcc }\n",
       "body { background: #f8f8f8; }\n",
       "body .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       "body .err { border: 1px solid #FF0000 } /* Error */\n",
       "body .k { color: #008000; font-weight: bold } /* Keyword */\n",
       "body .o { color: #666666 } /* Operator */\n",
       "body .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #9C6500 } /* Comment.Preproc */\n",
       "body .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       "body .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #A00000 } /* Generic.Deleted */\n",
       "body .ge { font-style: italic } /* Generic.Emph */\n",
       "body .gr { color: #E40000 } /* Generic.Error */\n",
       "body .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #008400 } /* Generic.Inserted */\n",
       "body .go { color: #717171 } /* Generic.Output */\n",
       "body .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       "body .gt { color: #0044DD } /* Generic.Traceback */\n",
       "body .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       "body .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       "body .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       "body .kp { color: #008000 } /* Keyword.Pseudo */\n",
       "body .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       "body .kt { color: #B00040 } /* Keyword.Type */\n",
       "body .m { color: #666666 } /* Literal.Number */\n",
       "body .s { color: #BA2121 } /* Literal.String */\n",
       "body .na { color: #687822 } /* Name.Attribute */\n",
       "body .nb { color: #008000 } /* Name.Builtin */\n",
       "body .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       "body .no { color: #880000 } /* Name.Constant */\n",
       "body .nd { color: #AA22FF } /* Name.Decorator */\n",
       "body .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       "body .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       "body .nf { color: #0000FF } /* Name.Function */\n",
       "body .nl { color: #767600 } /* Name.Label */\n",
       "body .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       "body .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       "body .nv { color: #19177C } /* Name.Variable */\n",
       "body .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       "body .w { color: #bbbbbb } /* Text.Whitespace */\n",
       "body .mb { color: #666666 } /* Literal.Number.Bin */\n",
       "body .mf { color: #666666 } /* Literal.Number.Float */\n",
       "body .mh { color: #666666 } /* Literal.Number.Hex */\n",
       "body .mi { color: #666666 } /* Literal.Number.Integer */\n",
       "body .mo { color: #666666 } /* Literal.Number.Oct */\n",
       "body .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       "body .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       "body .sc { color: #BA2121 } /* Literal.String.Char */\n",
       "body .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       "body .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       "body .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       "body .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       "body .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       "body .sx { color: #008000 } /* Literal.String.Other */\n",
       "body .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       "body .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       "body .ss { color: #19177C } /* Literal.String.Symbol */\n",
       "body .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #0000FF } /* Name.Function.Magic */\n",
       "body .vc { color: #19177C } /* Name.Variable.Class */\n",
       "body .vg { color: #19177C } /* Name.Variable.Global */\n",
       "body .vi { color: #19177C } /* Name.Variable.Instance */\n",
       "body .vm { color: #19177C } /* Name.Variable.Magic */\n",
       "body .il { color: #666666 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">class</span> <span class=\"nc\">TrivialVacuumEnvironment</span><span class=\"p\">(</span><span class=\"n\">Environment</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;This environment has two locations, A and B. Each can be Dirty</span>\n",
       "<span class=\"sd\">    or Clean. The agent perceives its location and the location&#39;s</span>\n",
       "<span class=\"sd\">    status. This serves as an example of how to implement a simple</span>\n",
       "<span class=\"sd\">    Environment.&quot;&quot;&quot;</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
       "        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span><span class=\"c1\">#para llamar los atributos de Environment</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">loc_A</span><span class=\"p\">:</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">([</span><span class=\"s1\">&#39;Clean&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Dirty&#39;</span><span class=\"p\">]),</span>\n",
       "                       <span class=\"n\">loc_B</span><span class=\"p\">:</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">([</span><span class=\"s1\">&#39;Clean&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Dirty&#39;</span><span class=\"p\">])}</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">thing_classes</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
       "        <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">Wall</span><span class=\"p\">,</span> <span class=\"n\">Dirt</span><span class=\"p\">,</span> <span class=\"n\">ReflexVacuumAgent</span><span class=\"p\">,</span> <span class=\"n\">RandomVacuumAgent</span><span class=\"p\">,</span> <span class=\"n\">TableDrivenVacuumAgent</span><span class=\"p\">,</span> <span class=\"n\">ModelBasedVacuumAgent</span><span class=\"p\">]</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">percept</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">agent</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Returns the agent&#39;s location, and the location status (Dirty/Clean).&quot;&quot;&quot;</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">location</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">status</span><span class=\"p\">[</span><span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">location</span><span class=\"p\">]</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">execute_action</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">agent</span><span class=\"p\">,</span> <span class=\"n\">action</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Change agent&#39;s location and/or location&#39;s status; track performance.</span>\n",
       "<span class=\"sd\">        Score 10 for each dirt cleaned; -1 for each move.&quot;&quot;&quot;</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">action</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;Right&#39;</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">location</span> <span class=\"o\">=</span> <span class=\"n\">loc_B</span>\n",
       "            <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">performance</span> <span class=\"o\">-=</span> <span class=\"mi\">1</span>\n",
       "        <span class=\"k\">elif</span> <span class=\"n\">action</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;Left&#39;</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">location</span> <span class=\"o\">=</span> <span class=\"n\">loc_A</span>\n",
       "            <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">performance</span> <span class=\"o\">-=</span> <span class=\"mi\">1</span>\n",
       "        <span class=\"k\">elif</span> <span class=\"n\">action</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;Suck&#39;</span><span class=\"p\">:</span>\n",
       "            <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">status</span><span class=\"p\">[</span><span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">location</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;Dirty&#39;</span><span class=\"p\">:</span>\n",
       "                <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">performance</span> <span class=\"o\">+=</span> <span class=\"mi\">10</span>\n",
       "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">status</span><span class=\"p\">[</span><span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">location</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;Clean&#39;</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">default_location</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">thing</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Agents start in either location at random.&quot;&quot;&quot;</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">([</span><span class=\"n\">loc_A</span><span class=\"p\">,</span> <span class=\"n\">loc_B</span><span class=\"p\">])</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agents import *\n",
    "from notebook import psource\n",
    "psource(TrivialVacuumEnvironment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91803c3e",
   "metadata": {},
   "source": [
    "La medición de rendimiento ya se tiene implementada como un atributo en la clase \"agent\" que se encuentra en el archivo agents.py. Esta implementación sirve para todos los distintos entornos (clases que se encuentra en el archivo agents.py) que se tienen creados dado a que heredan estos atributos de la clase \"agent\". Por lo tanto, se puede decir que la implementación para la medida del rendimiento es modular y sirve para los diferentes tipos de entorno.\n",
    "\n",
    "Con el objetivo de medir el rendimiento en el entorno de \"TrivialVacuumEnvironment()\" se creó un bucle \"for\". De esta forma, el agente se ubica inicialmente en la posición (0,0), realiza una acción aleatoria y se actualiza el estado del entorno. Para la siguiente iteración la nueva posición del agente está definido en función de la acción que se tomó en la iteración anterior, por lo que la variable posición se va actualizando para cada iteración. Esta variable de posición sirve como entrada para el comando trivial_vacuum_env.add_thing(agente,posicion).\n",
    "\n",
    "Como resultado se obtiene la información del estado inicial y final del entorno, ubicación inicial y final del agente, acción realizada en cada iteración, medida del rendimiento por iteración y medida del rendimiento total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2e865",
   "metadata": {},
   "source": [
    "## III. Ejercicio 2.11 del libro de texto de Russell & Norvig's 3ª edición.\n",
    "\n",
    "El ejercicio 2.11 plantea una versión modificada del entorno del agente \"aspiradora\" del ejercicio 2.8 del libro de texto de Russell & Norvig 3ª edición, en el cual la geografía del ambiente, es decir su extensión, límites y obstáculos son desconocidos, así como también se desconoce la configuración inicial de la suciedad. Ahora el agente puede moverse hacia arriba y hacia abajo, así como hacia la izquierda y derecha.\n",
    "\n",
    "Considerando dichas modificaciones se plantean las siguientes interrogantes, las cuales trataremos de responder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac5ba3",
   "metadata": {},
   "source": [
    "**1. ¿Puede un agente de reflejo simple ser perfectamente racional para este ambiente? Explique.**\n",
    "\n",
    "Los agentes reflejos simples tienen la admirable propiedad de ser simples, pero resultan tener una inteligencia limitada. Por ejemplo, la agente aspiradora tendrá un buen desempeño solo si toma la decisión correcta sabiendo únicamente de la percepción actual, es decir, solo si el entorno es completamente observable. De hecho, un poco de inobservabilidad puede provocar serios problemas. Por ejemplo, suponga un agente de aspiradora de reflejo simple solo posee un sensor de ubicación sino solo uno de suciedad, dicho agente solo puede percibir “Dirty” o “Clean”; él podría ejecutar la acción \"suck\" cuando este sucio, sin embargo, tendría dificultades para cambiar de ubicación y habría posibilidades de entrar en un bucle infinito si siempre elige moverse a la izquierda cuando se localiza en el cuadrado A, por ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904957c0",
   "metadata": {},
   "source": [
    "**2. ¿Puede un agente de reflejo simple con una función de agente aleatoria superar a un agente de reflejo simple? Diseñe tal agente y mida su rendimiento en varios ambientes.**\n",
    "\n",
    "\n",
    "Esto depende del entorno. Si este es complejo, impredecible o no observable, un agente reflejo simple con una función de agente aleatorio puede superar a un agente reflejo simple. Por otro si el entorno es observable, un agente reflejo simple con una función de agente aleatorio puede a lo mucho igualar a un agente simple. Esto se debe a que en un agente simple sus acciones están sujetas a condicionales, mientras que en el otro sus acciones son completamente aleatorias. Esto se comprueba en las dos implementaciones realizadas en las secciones A y B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6410a",
   "metadata": {},
   "source": [
    "### A. Agente de reflejo simple con una función de agente aleatoria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5de7f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "\n",
      "Iteracion: 1\n",
      "Estado inicial del entorno: {(0, 0): 'Dirty', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  ['Right']\n",
      "Estado final del entorno: {(0, 0): 'Dirty', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente despues de la accion (1, 0).\n",
      "Rendimiento -1.\n",
      "Rendimiento Total -1.\n",
      "===========================\n",
      "\n",
      "Iteracion: 2\n",
      "Estado inicial del entorno: {(0, 0): 'Dirty', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente antes de la accion (1, 0).\n",
      "Accion tomada:  ['Right', 'Left']\n",
      "Estado final del entorno: {(0, 0): 'Dirty', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente despues de la accion (0, 0).\n",
      "Rendimiento -1.\n",
      "Rendimiento Total -2.\n",
      "===========================\n",
      "\n",
      "Iteracion: 3\n",
      "Estado inicial del entorno: {(0, 0): 'Dirty', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  ['Right', 'Left', 'NoOp']\n",
      "Estado final del entorno: {(0, 0): 'Dirty', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente despues de la accion (0, 0).\n",
      "Rendimiento 0.\n",
      "Rendimiento Total -2.\n"
     ]
    }
   ],
   "source": [
    "#CON AGENTE RANDON\n",
    "\n",
    "# These are the two locations for the two-state environment\n",
    "loc_A, loc_B = (0, 0), (1, 0)\n",
    "# Initialize the two-state environment\n",
    "trivial_vacuum_env = TrivialVacuumEnvironment()\n",
    "\n",
    "# Create the random agent\n",
    "p=RandomAgentProgram(['Right', 'Left', 'Suck', 'NoOp'])#para la eleccion aleatoria\n",
    "random_agent = Agent(program=p)\n",
    "# These are the two locations for the two-state environment\n",
    "loc_A, loc_B = (0, 0), (1, 0)\n",
    "rendim=0\n",
    "\n",
    "trivial_vacuum_env.status={(0, 0): 'Dirty', (1, 0): 'Dirty'} # Para que inicie\n",
    "for i in range(3):     \n",
    "    \n",
    "    print('===========================')\n",
    "    print(\"\\nIteracion: %s\"%(i+1) )\n",
    "    p=RandomAgentProgram(['Right', 'Left', 'Suck', 'NoOp'])#para la eleccion aleatoria   \n",
    "\n",
    "    random_agent = Agent(program=p)\n",
    "    if i ==0:\n",
    "        posicion=(0,0)# El agente comienza en esta posicion \n",
    "    \n",
    "    trivial_vacuum_env.add_thing(random_agent,posicion) # Esta agregando el thing (randon_agent) al enviroment  \n",
    "    print(\"Estado inicial del entorno: {}.\".format(trivial_vacuum_env.status))    # Add agent to the environment\n",
    "    print(\"Ubicacion del agente antes de la accion {}.\".format(random_agent.location))    \n",
    "    # Running the environment\n",
    "    trivial_vacuum_env.step()  \n",
    "  \n",
    "    # Check the current state of the environment\n",
    "    print(\"Estado final del entorno: {}.\".format(trivial_vacuum_env.status))\n",
    "    print(\"Ubicacion del agente despues de la accion {}.\".format(random_agent.location))\n",
    "    print(\"Rendimiento {}.\".format(random_agent.performance))#añadido\n",
    "    \n",
    "    rendim+=random_agent.performance\n",
    "    print(\"Rendimiento Total {}.\".format(rendim))#añadido\n",
    "    \n",
    "    posicion=random_agent.location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193032f8",
   "metadata": {},
   "source": [
    "### B. Agente de reflejo simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ee8aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "\n",
      "Iteracion: 1\n",
      "Estado inicial del entorno: {(0, 0): 'Dirty', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  ['Right', 'Left', 'Suck']\n",
      "Estado final del entorno: {(0, 0): 'Clean', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente despues de la accion (0, 0).\n",
      "Rendimiento 10.\n",
      "===========================\n",
      "\n",
      "Iteracion: 2\n",
      "Can't add the same thing twice\n",
      "Estado inicial del entorno: {(0, 0): 'Clean', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  ['Right', 'Left', 'Right']\n",
      "Estado final del entorno: {(0, 0): 'Clean', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente despues de la accion (1, 0).\n",
      "Rendimiento 9.\n",
      "===========================\n",
      "\n",
      "Iteracion: 3\n",
      "Can't add the same thing twice\n",
      "Estado inicial del entorno: {(0, 0): 'Clean', (1, 0): 'Dirty'}.\n",
      "Ubicacion del agente antes de la accion (1, 0).\n",
      "Accion tomada:  ['Right', 'Left', 'Suck']\n",
      "Estado final del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente despues de la accion (1, 0).\n",
      "Rendimiento 19.\n"
     ]
    }
   ],
   "source": [
    "#CON AGENTE SIMPLE REFLEX\n",
    "trivial_vacuum_env.delete_thing(random_agent)\n",
    "\"\"\"We change the simpleReflexAgentProgram so that it doesn't make use of the Rule class\"\"\"\n",
    "def SimpleReflexAgentProgram():\n",
    "    \"\"\"This agent takes action based solely on the percept. [Figure 2.10]\"\"\"\n",
    "    \n",
    "    def program(percept):\n",
    "        loc, status = percept\n",
    "        return ('Suck' if status == 'Dirty' \n",
    "                else'Right' if loc == loc_A \n",
    "                            else'Left')\n",
    "    return program\n",
    "\n",
    "        \n",
    "# Create a simple reflex agent the two-state environment\n",
    "program = SimpleReflexAgentProgram()\n",
    "simple_reflex_agent = Agent(program)\n",
    "\n",
    "#trivial_vacuum_env.add_thing(simple_reflex_agent,posicion)\n",
    "\n",
    "\n",
    "# These are the two locations for the two-state environment\n",
    "loc_A, loc_B = (0, 0), (1, 0)\n",
    "rendim=0\n",
    "\n",
    "trivial_vacuum_env.status={(0, 0): 'Dirty', (1, 0): 'Dirty'} # Para que inicie\n",
    "\n",
    "for i in range(3):     \n",
    "    \n",
    "    print('===========================')\n",
    "    print(\"\\nIteracion: %s\"%(i+1) )  \n",
    "\n",
    "    #simple_reflex_agent = Agent(program)\n",
    "    \n",
    "    if i ==0:\n",
    "        posicion=(0,0)# El agente comienza en esta posicion \n",
    "        simple_reflex_agent.performance=0\n",
    "    \n",
    "    trivial_vacuum_env.add_thing(simple_reflex_agent,posicion) # Esta agregando el thing (randon_agent) al enviroment\n",
    "    \n",
    "    print(\"Estado inicial del entorno: {}.\".format(trivial_vacuum_env.status))    # Add agent to the environment\n",
    "    print(\"Ubicacion del agente antes de la accion {}.\".format(simple_reflex_agent.location))    \n",
    "    # Running the environment\n",
    "    trivial_vacuum_env.step()\n",
    "    if i ==0:\n",
    "        simple_reflex_agent.performance=10\n",
    "  \n",
    "    # Check the current state of the environment\n",
    "    print(\"Estado final del entorno: {}.\".format(trivial_vacuum_env.status))\n",
    "    print(\"Ubicacion del agente despues de la accion {}.\".format(simple_reflex_agent.location))\n",
    "    print(\"Rendimiento {}.\".format(simple_reflex_agent.performance))#añadido\n",
    "    \n",
    "    rendim+=simple_reflex_agent.performance\n",
    "    #print(\"Rendimiento Total {}.\".format(rendim))#añadido\n",
    "    \n",
    "    posicion=simple_reflex_agent.location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100336e0",
   "metadata": {},
   "source": [
    "**3. ¿Puede diseñar un ambiente en el que su agente aleatorio tenga un mal desempeño? Muestre sus resultados.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577454f",
   "metadata": {},
   "source": [
    "Un claro ejemplo del mal desempeño del agente aleatorio se muestra en la simulación implementada en la sección A (Agente de reflejo simple con una función de agente aleatoria). Se muestra que el agente a pesar de haber tenido varias iteraciones no logro limpiar ni una localización, obteniendo un rendimiento de -2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0f8d7",
   "metadata": {},
   "source": [
    "**4. ¿Puede un agente de reflejo con estado superar a un agente de reflejo simple? Diseñe tal agente y mida su rendimiento en varios ambientes. ¿Puede diseñar un agente racional de este tipo?**\n",
    "\n",
    "Un agente de reflejo con estado puede tener un mejor rendimiento que un agente de reflejo simple ya que mantiene un registro de las ubicaciones previas y de las acciones llevadas a cabo. De esta forma, el agente no se dirige a la misma locación en más de una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "274eaf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "\n",
      "Iteracion: 1\n",
      "Estado inicial del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  ['Left']\n",
      "Estado final del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente despues de la accion (0, 0).\n",
      "Rendimiento -1.\n",
      "Rendimiento Total -1.\n",
      "===========================\n",
      "\n",
      "Iteracion: 2\n",
      "Estado inicial del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  [None, 'Right']\n",
      "Estado final del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente despues de la accion (1, 0).\n",
      "Rendimiento -1.\n",
      "Rendimiento Total -2.\n",
      "===========================\n",
      "\n",
      "Iteracion: 3\n",
      "Estado inicial del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  [None, None, 'Right']\n",
      "Estado final del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente despues de la accion (1, 0).\n",
      "Rendimiento -1.\n",
      "Rendimiento Total -3.\n",
      "===========================\n",
      "\n",
      "Iteracion: 4\n",
      "Estado inicial del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  [None, None, None, 'Left']\n",
      "Estado final del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente despues de la accion (0, 0).\n",
      "Rendimiento -1.\n",
      "Rendimiento Total -4.\n",
      "===========================\n",
      "\n",
      "Iteracion: 5\n",
      "Estado inicial del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente antes de la accion (0, 0).\n",
      "Accion tomada:  [None, None, None, None, 'Right']\n",
      "Estado final del entorno: {(0, 0): 'Clean', (1, 0): 'Clean'}.\n",
      "Ubicacion del agente despues de la accion (1, 0).\n",
      "Rendimiento -1.\n",
      "Rendimiento Total -5.\n"
     ]
    }
   ],
   "source": [
    "from agents import *\n",
    "\n",
    "# These are the two locations for the two-state environment\n",
    "loc_A, loc_B = (0, 0), (1, 0)\n",
    "\n",
    "# Initialize the two-state environment\n",
    "vacuum_env = TrivialVacuumEnvironment()\n",
    "\n",
    "table = {((loc_A, 'Clean'),): 'Right',\n",
    "             ((loc_A, 'Dirty'),): 'Suck',\n",
    "             ((loc_B, 'Clean'),): 'Left',\n",
    "             ((loc_B, 'Dirty'),): 'Suck',\n",
    "             ((loc_A, 'Dirty'), (loc_A, 'Clean')): 'Right',\n",
    "             ((loc_A, 'Clean'), (loc_B, 'Dirty')): 'Suck',\n",
    "             ((loc_B, 'Clean'), (loc_A, 'Dirty')): 'Suck',\n",
    "             ((loc_B, 'Dirty'), (loc_B, 'Clean')): 'Left',\n",
    "             ((loc_A, 'Dirty'), (loc_A, 'Clean'), (loc_B, 'Dirty')): 'Suck',\n",
    "             ((loc_B, 'Dirty'), (loc_B, 'Clean'), (loc_A, 'Dirty')): 'Suck'\n",
    "        }\n",
    "\n",
    "rendimiento=0\n",
    "for i in range(5):   \n",
    "    print('===========================')\n",
    "    print(\"\\nIteracion: %s\"%(i+1) )\n",
    "    # Create a table-driven agent\n",
    "    table_driven_agent = Agent(program=TableDrivenAgentProgram(table=table))\n",
    "\n",
    "    # Add the table-driven agent to the environment\n",
    "    vacuum_env.add_thing(table_driven_agent)\n",
    "\n",
    "    print(\"Estado inicial del entorno: {}.\".format(vacuum_env.status))\n",
    "    print(\"Ubicacion del agente antes de la accion {}.\".format(random_agent.location))\n",
    "\n",
    "    # Run the environment\n",
    "    vacuum_env.step()\n",
    "\n",
    "    # Check the current state of the environment\n",
    "    print(\"Estado final del entorno: {}.\".format(vacuum_env.status))\n",
    "    print(\"Ubicacion del agente despues de la accion {}.\".format(table_driven_agent.location))\n",
    "    print(\"Rendimiento {}.\".format(table_driven_agent.performance))#añadido\n",
    "    \n",
    "    rendimiento+=table_driven_agent.performance \n",
    "    print(\"Rendimiento Total {}.\".format(rendimiento))#añadido\n",
    "    posicion=table_driven_agent.location\n",
    "    if(rendimiento>0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4fac43",
   "metadata": {},
   "source": [
    "## IV. Discusiones\n",
    "\n",
    "En la implementación del agente de reflejo simple y del agente de reflejo con una función aleatoria se observó que el que tuvo un mejor rendimiento fue el primero. Esto debido a que para que agente de reflejo simple tome sus acciones, este sigue algunas condicionales; cosa que no sucede en el agente con función aleatoria dado a que sus acciones son completamente aleatorias. Esto se observa con más detalle en la medida de los rendimientos. Se observa que el agente con función aleatoria luego de las iteraciones no logro limpiar nada y obtuvo un rendimiento de -2, mientras que el agente simple luego de la misma cantidad de iteraciones logro limpiar todas las posiciones logrando cumplir su objetivo y obteniendo un rendimiento de 19.\n",
    "\n",
    "Por otro lado, la desventaja que sufre un agente simple es su falta de memoria. En este punto un agente de reflejo con estado puede tener un mejor rendimiento que un agente de reflejo simple ya que mantiene un registro de las ubicaciones previas y de las acciones llevadas a cabo. De esta forma, el agente no se dirige a la misma locación en más de una vez.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac256ecf",
   "metadata": {},
   "source": [
    "## V. Conclusiones\n",
    "\n",
    "En este proyecto, se exploró el concepto de agentes reflejos, que son agentes simples que perciben su entorno a través de sensores y actúan sobre ese entorno a través de actuadores basados en un conjunto de reglas o reglas de condición-acción. Vimos que, si bien los agentes reflejos son fáciles de implementar, es posible que no siempre sean efectivos en todos los entornos, especialmente si el entorno es complejo o impredecible.\n",
    "\n",
    "Para abordar algunas de las limitaciones de los agentes reflejos simples, observamos dos extensiones: agentes reflejos aleatorios que toman decisiones aleatorias basadas en un conjunto de reglas y agentes reflejos con estado que mantienen un estado interno para tomar decisiones basadas en observaciones pasadas. Implementamos estas extensiones utilizando el módulo agent.py del repositorio de aimacode y probamos su rendimiento en diferentes entornos. Tambien se observó que los agentes reflejos aleatorios pueden superar a los agentes reflejos simples en algunas situaciones, especialmente si el entorno es impredecible y el agente puede beneficiarse explorando y experimentando con diferentes acciones. Sin embargo, los agentes reflejos aleatorios también pueden tener un desempeño deficiente si sus acciones aleatorias conducen a resultados subóptimos.\n",
    "\n",
    "Los agentes reflejos con estado, por otro lado, pueden mejorar su desempeño manteniendo un estado interno y tomando decisiones basadas en observaciones pasadas. Sin embargo, el diseño de un agente reflejo basado en el estado óptimo puede requerir una cantidad significativa de conocimiento y experimentación del dominio.\n",
    "En general, los agentes reflejos son un punto de partida simple y útil para crear agentes inteligentes, pero es posible que se necesiten técnicas más avanzadas, como la búsqueda, la planificación y el aprendizaje, para crear agentes que puedan funcionar bien en entornos complejos e impredecibles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfeaf13",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "[1] S. Russell, P. Norvig, “Artificial Intelligence”, 3st ed., Pearson, Ed. Pearson, 2010.\n",
    "\n",
    "[2] UC Berkeley code repository, “aimacode” https://github.com/aimacode, (accessed: 03.03.2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddd16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ddeca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
